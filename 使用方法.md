# æ•°æ®é›†

> å…ˆè¦æŠŠæ•°æ®é›†æ”¾å…¥datasetä¸­ï¼Œä¿®æ”¹data/ç›®å½•ä¸‹çš„yamlï¼Œè°ƒæ•´ä¸ºè‡ªå·±çš„æ•°æ®é›†ï¼Œéœ€è¦è°ƒæ•´è·¯å¾„ï¼Œåˆ†ç±»æ•°ï¼Œæ ‡ç­¾å

> yoloæ•°æ®é›†æ ¼å¼(yolov5çš„coco128å’Œéœ¹é›³å§å•¦Wzçš„yolo3ä¸ºä¾‹)
>
> txtå†…å®¹ï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯ `3 0.933536 0.486124 0.030408 0.154487`
>
> æ˜¯ label ä¸­å¿ƒæ¨ªåæ ‡ä¸å›¾åƒå®½åº¦æ¯”å€¼ ä¸­å¿ƒçºµåæ ‡ä¸å›¾åƒé«˜åº¦æ¯”å€¼ bboxå®½åº¦ä¸å›¾åƒå®½åº¦æ¯”å€¼ bboxé«˜åº¦ä¸å›¾åƒå®½é«˜æ¯”å€¼

```
#-------------------------------------------#
# 	yolov5çš„coco128æ ¼å¼
# 	éœ€è¦åœ¨~data/coco128.yamlä¸­ä¿®æ”¹å¦‚ä¸‹ä¿¡æ¯
# 	nc: 10  # åˆ†ç±»æ•°è¦å’Œdatasetä¸­ä¸€è‡´
# 	names: ["aeroplane", "bicycle", "bird", "boat", "bottle": 5] # åˆ†ç±»åç§°
#-------------------------------------------#
datasets
â”œâ”€â”€ coco128
	â”œâ”€â”€ images
    â”‚	â”œâ”€â”€ train2017	è®­ç»ƒå›¾ç‰‡
    â”‚	â””â”€â”€ val2017		éªŒè¯å›¾ç‰‡
	â””â”€â”€ labels
    	â”œâ”€â”€ train2017	è®­ç»ƒæ ‡ç­¾txt
    	â””â”€â”€ val2017		éªŒè¯æ ‡ç­¾txt


#-------------------------------------------#
#	éœ¹é›³å§å•¦Wzçš„yolo3
#-------------------------------------------#
data
â”œâ”€â”€ pascal_voc_classes.json		å­˜æ”¾ç±»åˆ«ä¿¡æ¯ {"aeroplane": 1, "bicycle": 2, "bird": 3, "boat": 4, "bottle": 5}
â”œâ”€â”€ train
â”‚	â”œâ”€â”€ images		è®­ç»ƒå›¾ç‰‡
â”‚	â””â”€â”€ labels		è®­ç»ƒæ ‡ç­¾txt
â””â”€â”€ val
	â”œâ”€â”€ images		éªŒè¯å›¾ç‰‡
	â””â”€â”€ labels		éªŒè¯å›¾ç‰‡txt
```

> data/class10.yaml

```yaml
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
# COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017) by Ultralytics
# Example usage: python train.py --data coco128.yaml
# parent
# â”œâ”€â”€ yolov5
# â””â”€â”€ datasets
#     â””â”€â”€ yourname
#         â””â”€â”€ images/
#             â””â”€â”€ train2017/  å­˜æ”¾è®­ç»ƒå›¾ç‰‡
#             â””â”€â”€ val2017/    å­˜æ”¾éªŒè¯å›¾ç‰‡
#         â””â”€â”€ labels/
#             â””â”€â”€ train2017/  å­˜æ”¾è®­ç»ƒæ ‡ç­¾  class x_center y_center width height
#             â””â”€â”€ val2017/    å­˜æ”¾éªŒè¯æ ‡ç­¾


# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ./datasets/coco128  # dataset root dir
train: images/train2017  # train images (relative to 'path') 128 images
val: images/val2017  # val images (relative to 'path') 128 images
test:  # test images (optional)

# Classes
nc: 10  # åˆ†ç±»æ•°è¦å’Œdatasetä¸­ä¸€è‡´
names: [
  "breakglue",
  "dot",
  "double",
  "gap",
  "mulglue",
  "noglue",
  "pit",
  "press",
  "reverse",
  "scratch"
]  # class names
```

# æ¨¡å‹

> ç„¶ååœ¨`models/yolov5*.yaml`ä¸­è®¾ç½®ç›¸åŒçš„åˆ†ç±»æ•°

```yaml
# Parameters
nc: 10  # è°ƒæ•´ä¸ºè‡ªå·±çš„åˆ†ç±»æ•°
```

# ä¸‹è½½æƒé‡

> å°†ä¸‹è½½å¥½çš„æƒé‡æ”¾åˆ°`weights/`æ–‡ä»¶ä¸‹ä¸‹

# Pretrained Checkpoints

| Model                                                        | size (pixels) | mAPval 0.5:0.95 | mAPval 0.5    | Speed CPU b1 (ms) | Speed V100 b1 (ms) | Speed V100 b32 (ms) | params (M) | FLOPs @640 (B) |
| ------------------------------------------------------------ | ------------- | --------------- | ------------- | ----------------- | ------------------ | ------------------- | ---------- | -------------- |
| [YOLOv5n](https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5n.pt) | 640           | 28.0            | 45.7          | **45**            | **6.3**            | **0.6**             | **1.9**    | **4.5**        |
| [YOLOv5s](https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt) | 640           | 37.4            | 56.8          | 98                | 6.4                | 0.9                 | 7.2        | 16.5           |
| [YOLOv5m](https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5m.pt) | 640           | 45.4            | 64.1          | 224               | 8.2                | 1.7                 | 21.2       | 49.0           |
| [YOLOv5l](https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5l.pt) | 640           | 49.0            | 67.3          | 430               | 10.1               | 2.7                 | 46.5       | 109.1          |
| [YOLOv5x](https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5x.pt) | 640           | 50.7            | 68.9          | 766               | 12.1               | 4.8                 | 86.7       | 205.7          |
|                                                              |               |                 |               |                   |                    |                     |            |                |
| [YOLOv5n6](https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5n6.pt) | 1280          | 36.0            | 54.4          | 153               | 8.1                | 2.1                 | 3.2        | 4.6            |
| [YOLOv5s6](https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s6.pt) | 1280          | 44.8            | 63.7          | 385               | 8.2                | 3.6                 | 12.6       | 16.8           |
| [YOLOv5m6](https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5m6.pt) | 1280          | 51.3            | 69.3          | 887               | 11.1               | 6.8                 | 35.7       | 50.0           |
| [YOLOv5l6](https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5l6.pt) | 1280          | 53.7            | 71.3          | 1784              | 15.8               | 10.5                | 76.8       | 111.4          |
| [YOLOv5x6](https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5x6.pt) + [TTA](https://github.com/ultralytics/yolov5/issues/303) | 1280 1536     | 55.0 **55.8**   | 72.7 **72.7** | 3136 -            | 26.2 -             | 19.4 -              | 140.7 -    | 209.8 -        |

<details open="">
  <summary>Table Notes (click to expand)</summary>
<ul dir="auto">
<li>All checkpoints are trained to 300 epochs with default settings. Nano and Small models use <a href="https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-low.yaml">hyp.scratch-low.yaml</a> hyps, all others use <a href="https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-high.yaml">hyp.scratch-high.yaml</a>.</li>
<li><strong>mAP<sup>val</sup></strong> values are for single-model single-scale on <a href="http://cocodataset.org" rel="nofollow">COCO val2017</a> dataset.<br>Reproduce by <code>python val.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65</code></li>
<li><strong>Speed</strong> averaged over COCO val images using a <a href="https://aws.amazon.com/ec2/instance-types/p3/" rel="nofollow">AWS p3.2xlarge</a> instance. NMS times (~1 ms/img) not included.<br>Reproduce by <code>python val.py --data coco.yaml --img 640 --task speed --batch 1</code></li>
<li><strong>TTA</strong> <a href="https://github.com/ultralytics/yolov5/issues/303" data-hovercard-type="issue" data-hovercard-url="/ultralytics/yolov5/issues/303/hovercard">Test Time Augmentation</a> includes reflection and scale augmentations.<br>Reproduce by <code>python val.py --data coco.yaml --img 1536 --iou 0.7 --augment</code></li>
</ul>
</details>

# æ˜¾å¡è®­ç»ƒ

```python
"""
Train a YOLOv5 model on a custom dataset.
Models and datasets download automatically from the latest YOLOv5 release.

Usage - Single-GPU training:
    $ python train.py --data coco128.yaml --weights yolov5s.pt --img 640  # from pretrained (recommended)
    $ python train.py --data coco128.yaml --weights '' --cfg yolov5s.yaml --img 640  # from scratch

Usage - Multi-GPU DDP training:
    $ python -m torch.distributed.run --nproc_per_node 4 --master_port 1 train.py --data coco128.yaml --weights yolov5s.pt --img 640 --device 0,1,2,3

Models:     https://github.com/ultralytics/yolov5/tree/master/models
Datasets:   https://github.com/ultralytics/yolov5/tree/master/data
Tutorial:   https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data
"""

def parse_opt(known=False):
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')
    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')
    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')
    parser.add_argument('--epochs', type=int, default=100, help='total training epochs')
    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')
    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')
    parser.add_argument('--rect', action='store_true', help='rectangular training')
    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')
    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')
    parser.add_argument('--noval', action='store_true', help='only validate final epoch')
    parser.add_argument('--noautoanchor', action='store_true', help='disable AutoAnchor')
    parser.add_argument('--noplots', action='store_true', help='save no plot files')
    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')
    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')
    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='image --cache ram/disk')
    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')
    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')
    parser.add_argument('--optimizer', type=str, choices=['SGD', 'Adam', 'AdamW'], default='SGD', help='optimizer')
    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')
    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')
    parser.add_argument('--project', default=ROOT / 'runs/train', help='save to project/name')
    parser.add_argument('--name', default='exp', help='save to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--quad', action='store_true', help='quad dataloader')
    parser.add_argument('--cos-lr', action='store_true', help='cosine LR scheduler')
    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')
    parser.add_argument('--patience', type=int, default=100, help='EarlyStopping patience (epochs without improvement)')
    parser.add_argument('--freeze', nargs='+', type=int, default=[0], help='Freeze layers: backbone=10, first3=0 1 2')
    parser.add_argument('--save-period', type=int, default=-1, help='Save checkpoint every x epochs (disabled if < 1)')
    parser.add_argument('--seed', type=int, default=0, help='Global training seed')
    parser.add_argument('--local_rank', type=int, default=-1, help='Automatic DDP Multi-GPU argument, do not modify')

    # Logger arguments
    parser.add_argument('--entity', default=None, help='Entity')
    parser.add_argument('--upload_dataset', nargs='?', const=True, default=False, help='Upload data, "val" option')
    parser.add_argument('--bbox_interval', type=int, default=-1, help='Set bounding-box image logging interval')
    parser.add_argument('--artifact_alias', type=str, default='latest', help='Version of dataset artifact to use')

    return parser.parse_known_args()[0] if known else parser.parse_args()
```


## å•æ˜¾å¡è®­ç»ƒ

> ex:

```sh
python train.py --img 640 --batch-size -1 --epochs 300 --device 0 --cos-lr --hyp data/hyps/hyp.scratch-low.yaml --optimizer SGD --weights weights/yolov5n.pt --cfg models/yolov5n.yaml --data data/coco128.yaml

python train.py --img 640 --batch-size -1 --epochs 300 --device 0 --cos-lr --hyp data/hyps/hyp.scratch-low.yaml --optimizer SGD --weights weights/yolov5s.pt --cfg models/yolov5s.yaml --data data/coco128.yaml

python train.py --img 640 --batch-size -1 --epochs 300 --device 0 --cos-lr --hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --weights weights/yolov5m.pt --cfg models/yolov5m.yaml --data data/coco128.yaml

python train.py --img 640 --batch-size -1 --epochs 300 --device 0 --cos-lr --hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --weights weights/yolov5l.pt --cfg models/yolov5l.yaml --data data/coco128.yaml

python train.py --img 640 --batch-size -1 --epochs 300 --device 0 --cos-lr --hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --weights weights/yolov5x.pt --cfg models/yolov5x.yaml --data data/coco128.yaml
```

## å¤šæ˜¾å¡è®­ç»ƒ

- -m torch.distributed.launch pytorchå¯ç”¨å¤šçº¿ç¨‹
- --nproc_per_node=2 2å¼ æ˜¾å¡
- --device 0,1     2å¼ æ˜¾å¡åºå·

```sh
python -m torch.distributed.launch --nproc_per_node=2 train.py --img 640 --batch-size -1 --epochs 300 --device 0,1 --cos-lr \
--hyp data/hyps/hyp.scratch-low.yaml --optimizer SGD --weights weights/yolov5n.pt --cfg models/yolov5n.yaml \
--data data/coco128.yaml

python -m torch.distributed.launch --nproc_per_node=2 train.py --img 640 --batch-size -1 --epochs 300 --device 0,1 --cos-lr \
--hyp data/hyps/hyp.scratch-low.yaml --optimizer SGD --weights weights/yolov5s.pt --cfg models/yolov5s.yaml \
--data data/coco128.yaml

python -m torch.distributed.launch --nproc_per_node=2 train.py --img 640 --batch-size -1 --epochs 300 --device 0,1 --cos-lr \
--hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --weights weights/yolov5m.pt --cfg models/yolov5m.yaml \
--data data/coco128.yaml

python -m torch.distributed.launch --nproc_per_node=2 train.py --img 640 --batch-size -1 --epochs 300 --device 0,1 --cos-lr \
--hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --weights weights/yolov5l.pt --cfg models/yolov5l.yaml \
--data data/coco128.yaml

python -m torch.distributed.launch --nproc_per_node=2 train.py --img 640 --batch-size -1 --epochs 300 --device 0,1 --cos-lr \
--hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --weights weights/yolov5x.pt --cfg models/yolov5x.yaml \
--data data/coco128.yaml
```

# export

```python
"""
Export a YOLOv5 PyTorch model to other formats. TensorFlow exports authored by https://github.com/zldrobit

Format                      | `export.py --include`         | Model
---                         | ---                           | ---
PyTorch                     | -                             | yolov5s.pt
TorchScript                 | `torchscript`                 | yolov5s.torchscript
ONNX                        | `onnx`                        | yolov5s.onnx
OpenVINO                    | `openvino`                    | yolov5s_openvino_model/
TensorRT                    | `engine`                      | yolov5s.engine
CoreML                      | `coreml`                      | yolov5s.mlmodel
TensorFlow SavedModel       | `saved_model`                 | yolov5s_saved_model/
TensorFlow GraphDef         | `pb`                          | yolov5s.pb
TensorFlow Lite             | `tflite`                      | yolov5s.tflite
TensorFlow Edge TPU         | `edgetpu`                     | yolov5s_edgetpu.tflite
TensorFlow.js               | `tfjs`                        | yolov5s_web_model/
PaddlePaddle                | `paddle`                      | yolov5s_paddle_model/

Requirements:
    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU
    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow  # GPU

Usage:
    $ python export.py --weights yolov5s.pt --include torchscript onnx openvino engine coreml tflite ...

Inference:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle

TensorFlow.js:
    $ cd .. && git clone https://github.com/zldrobit/tfjs-yolov5-example.git && cd tfjs-yolov5-example
    $ npm install
    $ ln -s ../../yolov5/yolov5s_web_model public/yolov5s_web_model
    $ npm start
"""

def parse_opt(known=False):
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model.pt path(s)')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640, 640], help='image (h, w)')
    parser.add_argument('--batch-size', type=int, default=1, help='batch size')
    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')
    parser.add_argument('--inplace', action='store_true', help='set YOLOv5 Detect() inplace=True')
    parser.add_argument('--keras', action='store_true', help='TF: use Keras')
    parser.add_argument('--optimize', action='store_true', help='TorchScript: optimize for mobile')
    parser.add_argument('--int8', action='store_true', help='CoreML/TF INT8 quantization')
    parser.add_argument('--dynamic', action='store_true', help='ONNX/TF/TensorRT: dynamic axes')
    parser.add_argument('--simplify', action='store_true', help='ONNX: simplify model')
    parser.add_argument('--opset', type=int, default=17, help='ONNX: opset version')
    parser.add_argument('--verbose', action='store_true', help='TensorRT: verbose log')
    parser.add_argument('--workspace', type=int, default=4, help='TensorRT: workspace size (GB)')
    parser.add_argument('--nms', action='store_true', help='TF: add NMS to model')
    parser.add_argument('--agnostic-nms', action='store_true', help='TF: add agnostic NMS to model')
    parser.add_argument('--topk-per-class', type=int, default=100, help='TF.js NMS: topk per class to keep')
    parser.add_argument('--topk-all', type=int, default=100, help='TF.js NMS: topk for all classes to keep')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='TF.js NMS: IoU threshold')
    parser.add_argument('--conf-thres', type=float, default=0.25, help='TF.js NMS: confidence threshold')
    parser.add_argument(
        '--include',
        nargs='+',
        default=['torchscript'],
        help='torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle')
    opt = parser.parse_known_args()[0] if known else parser.parse_args()
    print_args(vars(opt))
    return opt
```

> å¯¼å‡ºè·¯å¾„å’Œæƒé‡è·¯å¾„ç›¸åŒ
>
> --include åé¢å†™æƒ³å¯¼å‡ºçš„æ ¼å¼

## torchscript

```python
python export.py --weights weights/yolov5s.pt --include torchscript --device 0
```

## onnx

> æ³¨æ„:
>
> `onnxruntime` å’Œ `onnxruntime-gpu` ä¸è¦åŒæ—¶å®‰è£…ï¼Œå¦åˆ™ä½¿ç”¨ `gpu` æ¨ç†æ—¶é€Ÿåº¦ä¼šå¾ˆæ…¢ï¼Œå¦‚æœåŒæ—¶å®‰è£…äº†2ä¸ªåŒ…ï¼Œè¦å…¨éƒ¨å¸è½½ï¼Œå†å®‰è£… 'onnxruntime-gpu' æ‰èƒ½ä½¿ç”¨gpuæ¨ç†ï¼Œå¦åˆ™gpué€Ÿåº¦ä¼šå¾ˆæ…¢

```python
python export.py --weights weights/yolov5s.pt --include onnx --simplify
```

## openvino

```python
python export.py --weights weights/yolov5s.pt --include openvino
```

### export failure  0.9s: DLL load failed while importing ie_api

> https://blog.csdn.net/qq_26815239/article/details/123047840
>
> å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œå¹¶ä¸”æ˜¯åœ¨Windowsç³»ç»Ÿä¸‹é€šè¿‡pipå®‰è£…çš„openvinoï¼Œé‚£ä¹ˆè¯¥é”™è¯¯çš„è§£å†³æ–¹æ¡ˆå¦‚ä¸‹ï¼š

1. è¿›å…¥ç›®å½• `your\env\site-packages\openvino\inference_engine`
2. æ‰“å¼€æ–‡ä»¶ `__init__.py`
3. 26è¡Œä¸‹æ·»åŠ ä¸€è¡Œ

```python
        if os.path.isdir(lib_path):
            # On Windows, with Python >= 3.8, DLLs are no longer imported from the PATH.
            if (3, 8) <= sys.version_info:
                os.add_dll_directory(os.path.abspath(lib_path))
                os.environ['PATH'] = os.path.abspath(lib_path) + ';' + os.environ['PATH']	# æ·»åŠ è¿™ä¸€è¡Œ
```

## tensorrt

```python
python export.py --weights weights/yolov5s.pt --include engine --device 0 --half
```

## onnx openvino tensorrt

```
python export.py --weights weights/yolov5s.pt --include onnx openvino engine --simplify --device 0 --half
```

# detect

```python
"""
Run YOLOv5 detection inference on images, videos, directories, globs, YouTube, webcam, streams, etc.

Usage - sources:
    $ python detect.py --weights yolov5s.pt --source 0                               # webcam
                                                     img.jpg                         # image
                                                     vid.mp4                         # video
                                                     screen                          # screenshot
                                                     path/                           # directory
                                                     list.txt                        # list of images
                                                     list.streams                    # list of streams
                                                     'path/*.jpg'                    # glob
                                                     'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                                     'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle
"""

def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL')
    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob/screen/0(webcam)')
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')
    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='show results')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--visualize', action='store_true', help='visualize features')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')
    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')
    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    parser.add_argument('--vid-stride', type=int, default=1, help='video frame-rate stride')
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand
    print_args(vars(opt))
    return opt
```

> sourceåé¢å¯ä»¥æ”¾å›¾ç‰‡ï¼Œè§†é¢‘æˆ–è€…æ–‡ä»¶å¤¹è·¯å¾„ï¼Œä¼šä¿å­˜åˆ°runs/detectç›®å½•ä¸‹é¢

`python detect.py --weights æƒé‡è·¯å¾„ --source å›¾ç‰‡orè§†é¢‘oræ–‡ä»¶å¤¹è·¯å¾„`



```python
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Run YOLOv5 detection inference on images, videos, directories, globs, YouTube, webcam, streams, etc.

Usage - sources:
    $ python detect.py --weights yolov5s.pt --source 0                               # webcam
                                                     img.jpg                         # image
                                                     vid.mp4                         # video
                                                     screen                          # screenshot
                                                     path/                           # directory
                                                     list.txt                        # list of images
                                                     list.streams                    # list of streams
                                                     'path/*.jpg'                    # glob
                                                     'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                                     'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle
"""
```

## torch

```python
python detect.py --imgsz 640 --weights weights/yolov5s.pt --data data/coco128.yaml --source data/images/bus.jpg --device 0

python detect.py --imgsz 640--weights weights/yolov5s.pt --data data/coco128.yaml --source ../datasets/coco128/images/train2017 --device 0 
```

## torchscript

```python
python detect.py --imgsz 640 --weights weights/yolov5s.torchscript --data data/coco128.yaml --source data/images/bus.jpg --device 0

python detect.py --imgsz 640 --weights weights/yolov5s.torchscript --data data/coco128.yaml --source ../datasets/coco128/images/train2017 --device 0
```

## onnx

> æ³¨æ„:
>
> `onnxruntime` å’Œ `onnxruntime-gpu` ä¸è¦åŒæ—¶å®‰è£…ï¼Œå¦åˆ™ä½¿ç”¨ `gpu` æ¨ç†æ—¶é€Ÿåº¦ä¼šå¾ˆæ…¢ï¼Œå¦‚æœåŒæ—¶å®‰è£…äº†2ä¸ªåŒ…ï¼Œè¦å…¨éƒ¨å¸è½½ï¼Œå†å®‰è£… 'onnxruntime-gpu' æ‰èƒ½ä½¿ç”¨gpuæ¨ç†ï¼Œå¦åˆ™gpué€Ÿåº¦ä¼šå¾ˆæ…¢

```python
python detect.py --imgsz 640 --weights weights/yolov5s.onnx --data data/coco128.yaml --source data/images/bus.jpg --device 0

python detect.py --imgsz 640 --weights weights/yolov5s.onnx --data data/coco128.yaml --source ../datasets/coco128/images/train2017 --device 0
```

### opencv dnn

```python
torch.onnx.export(
    model.cpu() if dynamic else model,  # --dynamic only compatible with cpu
    im.cpu() if dynamic else im,
    f,
    verbose=False,
    opset_version=opset,
    do_constant_folding=True,  # WARNING: DNN inference with torch>=1.12 may require do_constant_folding=False
    input_names=['images'],
    output_names=output_names,
    dynamic_axes=dynamic or None)
```

```
python detect.py --imgsz 640 --weights weights/yolov5s.onnx --data data/coco128.yaml --source data/images/bus.jpg --dnn

python detect.py --imgsz 640 --weights weights/yolov5s.onnx --data data/coco128.yaml --source ../datasets/coco128/images/train2017 --dnn
```

## openvino

```python
python detect.py --imgsz 640 --weights weights/yolov5s_openvino_model --data data/coco128.yaml --source data/images/bus.jpg --device cpu

python detect.py --imgsz 640 --weights weights/yolov5s_openvino_model --data data/coco128.yaml --source ../datasets/coco128/images/train2017 --device cpu
```

## tensorrt

```python
python detect.py --imgsz 640 --half --weights weights/yolov5s.engine --data data/coco128.yaml --source data/images/bus.jpg --device 0

python detect.py --imgsz 640 --half --weights weights/yolov5s.engine --data data/coco128.yaml --source ../datasets/coco128/images/train2017 --device 0
```

# val

```python
"""
Validate a trained YOLOv5 detection model on a detection dataset

Usage:
    $ python val.py --weights yolov5s.pt --data coco128.yaml --img 640

Usage - formats:
    $ python val.py --weights yolov5s.pt                 # PyTorch
                              yolov5s.torchscript        # TorchScript
                              yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                              yolov5s_openvino_model     # OpenVINO
                              yolov5s.engine             # TensorRT
                              yolov5s.mlmodel            # CoreML (macOS-only)
                              yolov5s_saved_model        # TensorFlow SavedModel
                              yolov5s.pb                 # TensorFlow GraphDef
                              yolov5s.tflite             # TensorFlow Lite
                              yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                              yolov5s_paddle_model       # PaddlePaddle
"""

def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')
    parser.add_argument('--batch-size', type=int, default=32, help='batch size')
    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.001, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.6, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=300, help='maximum detections per image')
    parser.add_argument('--task', default='val', help='train, val, test, speed or study')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')
    parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--verbose', action='store_true', help='report mAP by class')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-hybrid', action='store_true', help='save label+prediction hybrid results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-json', action='store_true', help='save a COCO-JSON results file')
    parser.add_argument('--project', default=ROOT / 'runs/val', help='save to project/name')
    parser.add_argument('--name', default='exp', help='save to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    opt = parser.parse_args()
    opt.data = check_yaml(opt.data)  # check YAML
    opt.save_json |= opt.data.endswith('coco.yaml')
    opt.save_txt |= opt.save_hybrid
    print_args(vars(opt))
    return opt
```

## torch

```python
python val.py --imgsz 640 --save-txt --save-hybrid --save-conf --save-json --conf-thres 0.25 --iou-thres 0.45 \
--batch-size 32 --weights weights/yolov5s.pt --data data/coco128.yaml --device 0
```

## torchscript

```python
python val.py --imgsz 640 --save-txt --save-hybrid --save-conf --save-json --conf-thres 0.25 --iou-thres 0.4 \
--batch-size 32 --weights weights/yolov5s.torchscript --data data/coco128.yaml5 --device 0
```

## onnx

> æ³¨æ„:
>
> `onnxruntime` å’Œ `onnxruntime-gpu` ä¸è¦åŒæ—¶å®‰è£…ï¼Œå¦åˆ™ä½¿ç”¨ `gpu` æ¨ç†æ—¶é€Ÿåº¦ä¼šå¾ˆæ…¢ï¼Œå¦‚æœåŒæ—¶å®‰è£…äº†2ä¸ªåŒ…ï¼Œè¦å…¨éƒ¨å¸è½½ï¼Œå†å®‰è£… 'onnxruntime-gpu' æ‰èƒ½ä½¿ç”¨gpuæ¨ç†ï¼Œå¦åˆ™gpué€Ÿåº¦ä¼šå¾ˆæ…¢

```python
python val.py --imgsz 640 --save-txt --save-hybrid --save-conf --save-json --conf-thres 0.25 --iou-thres 0.45 \
--batch-size 32 --weights weights/yolov5s.onnx --data data/coco128.yaml --device 0

python val.py --imgsz 640 --save-txt --save-hybrid --save-conf --save-json --conf-thres 0.25 --iou-thres 0.45 \
--batch-size 32 --dnn --weights weights/yolov5s.onnx --data data/coco128.yaml --device 0
```

## openvino

```python
python val.py --imgsz 640 --save-txt --save-hybrid --save-conf --save-json --conf-thres 0.25 --iou-thres 0.45 \
--batch-size 32 --weights weights/yolov5s_openvino_model --data data/coco128.yaml --device cpu
```

## tensorrt

```python
python val.py --imgsz 640 --save-txt --save-hybrid --save-conf --save-json --conf-thres 0.25 --iou-thres 0.45 --half \
--batch-size 32 --weights weights/yolov5s.engine --data data/coco128.yaml --device 0
```

