[Ultralytics YOLOv5 - Ultralytics YOLOv8 Docs](https://docs.ultralytics.com/yolov5/)

# Êï∞ÊçÆÈõÜ

> ÂÖàË¶ÅÊääÊï∞ÊçÆÈõÜÊîæÂÖ•dataset‰∏≠Ôºå‰øÆÊîπdata/ÁõÆÂΩï‰∏ãÁöÑyamlÔºåË∞ÉÊï¥‰∏∫Ëá™Â∑±ÁöÑÊï∞ÊçÆÈõÜÔºåÈúÄË¶ÅË∞ÉÊï¥Ë∑ØÂæÑÔºåÂàÜÁ±ªÊï∞ÔºåÊ†áÁ≠æÂêç

> yoloÊï∞ÊçÆÈõÜÊ†ºÂºè(yolov5/v8ÁöÑcoco128ÂíåÈúπÈõ≥ÂêßÂï¶WzÁöÑyolo3‰∏∫‰æã)
>
> txtÂÜÖÂÆπÔºåÊØè‰∏ÄË°åÈÉΩÊòØ `3 0.933536 0.486124 0.030408 0.154487`
>
> ÊòØ label ‰∏≠ÂøÉÊ®™ÂùêÊ†á‰∏éÂõæÂÉèÂÆΩÂ∫¶ÊØîÂÄº ‰∏≠ÂøÉÁ∫µÂùêÊ†á‰∏éÂõæÂÉèÈ´òÂ∫¶ÊØîÂÄº bboxÂÆΩÂ∫¶‰∏éÂõæÂÉèÂÆΩÂ∫¶ÊØîÂÄº bboxÈ´òÂ∫¶‰∏éÂõæÂÉèÂÆΩÈ´òÊØîÂÄº

```sh
#-------------------------------------------#
# 	yolov5 v8ÁöÑÊ†ºÂºè
#-------------------------------------------#
yaml:
    path: ../datasets/coco128   # dataset root dir
    train: images/train2017     # train images (relative to 'path') 128 images
    val: images/train2017       # val images (relative to 'path') 128 images
    test:                       # test images (optional)
dir:
    datasets
    ‚îú‚îÄ‚îÄ coco128
        ‚îú‚îÄ‚îÄ images
        ‚îÇ   ‚îú‚îÄ‚îÄ train2017   # ËÆ≠ÁªÉÂõæÁâá
        ‚îÇ   ‚îî‚îÄ‚îÄ val2017     # È™åËØÅÂõæÁâá
        ‚îî‚îÄ‚îÄ labels
            ‚îú‚îÄ‚îÄ train2017   # ËÆ≠ÁªÉÊ†áÁ≠ætxt
            ‚îî‚îÄ‚îÄ val2017     # È™åËØÅÊ†áÁ≠ætxt

#-------------------------------------------#
# 	yolov5 v8Âè¶ÁöÑ‰∏ÄÁßçÂõæÁâáÁõÆÂΩïÊ†ºÂºè
#-------------------------------------------#
yaml:
    path: ../datasets/coco128   # dataset root dir
    train: train/images         # train images (relative to 'path')
    val: val/images             # val images (relative to 'path')
    test: test/images           # test images (optional)
dir:
    datasets
    ‚îú‚îÄ‚îÄ coco128
        ‚îú‚îÄ‚îÄ train
        ‚îÇ   ‚îú‚îÄ‚îÄ images  # ËÆ≠ÁªÉÂõæÁâá
        ‚îÇ   ‚îî‚îÄ‚îÄ labels  # ËÆ≠ÁªÉÊ†áÁ≠ætxt
        ‚îú‚îÄ‚îÄ val
        ‚îÇ   ‚îú‚îÄ‚îÄ images  # È™åËØÅÂõæÁâá
        ‚îÇ   ‚îî‚îÄ‚îÄ labels  # È™åËØÅÊ†áÁ≠ætxt
        ‚îî‚îÄ‚îÄ test
            ‚îú‚îÄ‚îÄ images  # ÊµãËØïÂõæÁâá
            ‚îî‚îÄ‚îÄ labels  # ÊµãËØïÊ†áÁ≠ætxt


#-------------------------------------------#
#	ÈúπÈõ≥ÂêßÂï¶WzÁöÑyolo3
#-------------------------------------------#
data
‚îú‚îÄ‚îÄ pascal_voc_classes.json		Â≠òÊîæÁ±ªÂà´‰ø°ÊÅØ {"aeroplane": 1, "bicycle": 2, "bird": 3, "boat": 4, "bottle": 5}
‚îú‚îÄ‚îÄ train
‚îÇ	‚îú‚îÄ‚îÄ images  # ËÆ≠ÁªÉÂõæÁâá
‚îÇ	‚îî‚îÄ‚îÄ labels  # ËÆ≠ÁªÉÊ†áÁ≠ætxt
‚îî‚îÄ‚îÄ val
	‚îú‚îÄ‚îÄ images  # È™åËØÅÂõæÁâá
	‚îî‚îÄ‚îÄ labels  # È™åËØÅÂõæÁâátxt
```

> `data/class20.yaml`

```yaml
# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license
# COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017) by Ultralytics
# Example usage: python train.py --data coco128.yaml
# parent
# ‚îú‚îÄ‚îÄ yolov5
# ‚îî‚îÄ‚îÄ datasets
#     ‚îî‚îÄ‚îÄ yourname
#         ‚îî‚îÄ‚îÄ images/
#             ‚îî‚îÄ‚îÄ train2017/  Â≠òÊîæËÆ≠ÁªÉÂõæÁâá
#             ‚îî‚îÄ‚îÄ val2017/    Â≠òÊîæÈ™åËØÅÂõæÁâá
#         ‚îî‚îÄ‚îÄ labels/
#             ‚îî‚îÄ‚îÄ train2017/  Â≠òÊîæËÆ≠ÁªÉÊ†áÁ≠æ  class x_center y_center width height
#             ‚îî‚îÄ‚îÄ val2017/    Â≠òÊîæÈ™åËØÅÊ†áÁ≠æ


# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ../datasets/classes20  # dataset root dir
train: images/train2017  # train images (relative to 'path') 128 images
val: images/val2017  # val images (relative to 'path') 128 images
test:  # test images (optional)

# Classes
names:
  0: aeroplane
  1: bicycle
  2: bird
  3: boat
  4: bottle
  5: bus
  6: car
  7: cat
  8: chair
  9: cow
  10: diningtable
  11: dog
  12: horse
  13: motorbike
  14: person
  15: pottedplant
  16: sheep
  17: sofa
  18: train
  19: tvmonitor
```

# Ê®°Âûã

# ‰∏ãËΩΩÊùÉÈáç

> Â∞Ü‰∏ãËΩΩÂ•ΩÁöÑÊùÉÈáçÊîæÂà∞`weights/`Êñá‰ª∂‰∏ã‰∏ã

### È¢ÑËÆ≠ÁªÉÊ®°Âûã

| Ê®°Âûã                                                         | Â∞∫ÂØ∏ ÔºàÂÉèÁ¥†Ôºâ | mAPval 50-95  | mAPval 50     | Êé®ÁêÜÈÄüÂ∫¶ CPU b1 ÔºàmsÔºâ | Êé®ÁêÜÈÄüÂ∫¶ V100 b1 ÔºàmsÔºâ | ÈÄüÂ∫¶ V100 b32 ÔºàmsÔºâ | ÂèÇÊï∞Èáè (M) | FLOPs @640 (B) |
| ------------------------------------------------------------ | ------------- | ------------- | ------------- | ---------------------- | ----------------------- | -------------------- | ---------- | -------------- |
| [YOLOv5n](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt) | 640           | 28.0          | 45.7          | **45**                 | **6.3**                 | **0.6**              | **1.9**    | **4.5**        |
| [YOLOv5s](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt) | 640           | 37.4          | 56.8          | 98                     | 6.4                     | 0.9                  | 7.2        | 16.5           |
| [YOLOv5m](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt) | 640           | 45.4          | 64.1          | 224                    | 8.2                     | 1.7                  | 21.2       | 49.0           |
| [YOLOv5l](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt) | 640           | 49.0          | 67.3          | 430                    | 10.1                    | 2.7                  | 46.5       | 109.1          |
| [YOLOv5x](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt) | 640           | 50.7          | 68.9          | 766                    | 12.1                    | 4.8                  | 86.7       | 205.7          |
|                                                              |               |               |               |                        |                         |                      |            |                |
| [YOLOv5n6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt) | 1280          | 36.0          | 54.4          | 153                    | 8.1                     | 2.1                  | 3.2        | 4.6            |
| [YOLOv5s6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s6.pt) | 1280          | 44.8          | 63.7          | 385                    | 8.2                     | 3.6                  | 12.6       | 16.8           |
| [YOLOv5m6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m6.pt) | 1280          | 51.3          | 69.3          | 887                    | 11.1                    | 6.8                  | 35.7       | 50.0           |
| [YOLOv5l6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l6.pt) | 1280          | 53.7          | 71.3          | 1784                   | 15.8                    | 10.5                 | 76.8       | 111.4          |
| [YOLOv5x6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x6.pt) +[TTA](https://github.com/ultralytics/yolov5/issues/303) | 1280 1536     | 55.0 **55.8** | 72.7 **72.7** | 3136 -                 | 26.2 -                  | 19.4 -               | 140.7 -    | 209.8 -        |

Á¨îËÆ∞

- ÊâÄÊúâÊ®°ÂûãÈÉΩ‰ΩøÁî®ÈªòËÆ§ÈÖçÁΩÆÔºåËÆ≠ÁªÉ 300 epochs„ÄÇnÂíåsÊ®°Âûã‰ΩøÁî® [hyp.scratch-low.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-low.yaml) ÔºåÂÖ∂‰ªñÊ®°ÂûãÈÉΩ‰ΩøÁî® [hyp.scratch-high.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-high.yaml) „ÄÇ
- **mAPval**Âú®ÂçïÊ®°ÂûãÂçïÂ∞∫Â∫¶‰∏äËÆ°ÁÆóÔºåÊï∞ÊçÆÈõÜ‰ΩøÁî® [COCO val2017](http://cocodataset.org/) „ÄÇ
  Â§çÁé∞ÂëΩ‰ª§ `python val.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65`
- **Êé®ÁêÜÈÄüÂ∫¶**Âú® COCO val ÂõæÂÉèÊÄª‰ΩìÊó∂Èó¥‰∏äËøõË°åÂπ≥ÂùáÂæóÂà∞ÔºåÊµãËØïÁéØÂ¢É‰ΩøÁî®[AWS p3.2xlarge](https://aws.amazon.com/ec2/instance-types/p3/)ÂÆû‰æã„ÄÇ NMS Êó∂Èó¥ (Â§ßÁ∫¶ 1 ms/img) ‰∏çÂåÖÊã¨Âú®ÂÜÖ„ÄÇ
  Â§çÁé∞ÂëΩ‰ª§ `python val.py --data coco.yaml --img 640 --task speed --batch 1`
- **TTA** [ÊµãËØïÊó∂Êï∞ÊçÆÂ¢ûÂº∫](https://github.com/ultralytics/yolov5/issues/303) ÂåÖÊã¨ÂèçÂ∞ÑÂíåÂ∞∫Â∫¶ÂèòÊç¢„ÄÇ
  Â§çÁé∞ÂëΩ‰ª§ `python val.py --data coco.yaml --img 1536 --iou 0.7 --augment`

# ÊòæÂç°ËÆ≠ÁªÉ

> [yolov5‚Äî‚ÄîËÆ≠ÁªÉÁ≠ñÁï•](https://blog.csdn.net/CharmsLUO/article/details/123577851)

> [Train Custom Data Tutorial ‚≠ê ¬∑ Issue #12 ¬∑ ultralytics/yolov5 (github.com)](https://github.com/ultralytics/yolov5/issues/12)

```python
"""
Train a YOLOv5 model on a custom dataset.
Models and datasets download automatically from the latest YOLOv5 release.

Usage - Single-GPU training:
    $ python train.py --data coco128.yaml --weights yolov5s.pt --img 640  # from pretrained (recommended)
    $ python train.py --data coco128.yaml --weights '' --cfg yolov5s.yaml --img 640  # from scratch

Usage - Multi-GPU DDP training:
    $ python -m torch.distributed.run --nproc_per_node 4 --master_port 1 train.py --data coco128.yaml --weights yolov5s.pt --img 640 --device 0,1,2,3

Models:     https://github.com/ultralytics/yolov5/tree/master/models
Datasets:   https://github.com/ultralytics/yolov5/tree/master/data
Tutorial:   https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data
"""

def parse_opt(known=False):
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')
    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')
    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')
    parser.add_argument('--epochs', type=int, default=100, help='total training epochs')
    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')
    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')
    parser.add_argument('--rect', action='store_true', help='rectangular training')
    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')
    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')
    parser.add_argument('--noval', action='store_true', help='only validate final epoch')
    parser.add_argument('--noautoanchor', action='store_true', help='disable AutoAnchor')
    parser.add_argument('--noplots', action='store_true', help='save no plot files')
    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')
    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')
    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='image --cache ram/disk')
    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')
    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')
    parser.add_argument('--optimizer', type=str, choices=['SGD', 'Adam', 'AdamW'], default='SGD', help='optimizer')
    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')
    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')
    parser.add_argument('--project', default=ROOT / 'runs/train', help='save to project/name')
    parser.add_argument('--name', default='exp', help='save to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--quad', action='store_true', help='quad dataloader')
    parser.add_argument('--cos-lr', action='store_true', help='cosine LR scheduler')
    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')
    parser.add_argument('--patience', type=int, default=100, help='EarlyStopping patience (epochs without improvement)')
    parser.add_argument('--freeze', nargs='+', type=int, default=[0], help='Freeze layers: backbone=10, first3=0 1 2')
    parser.add_argument('--save-period', type=int, default=-1, help='Save checkpoint every x epochs (disabled if < 1)')
    parser.add_argument('--seed', type=int, default=0, help='Global training seed')
    parser.add_argument('--local_rank', type=int, default=-1, help='Automatic DDP Multi-GPU argument, do not modify')

    # Logger arguments
    parser.add_argument('--entity', default=None, help='Entity')
    parser.add_argument('--upload_dataset', nargs='?', const=True, default=False, help='Upload data, "val" option')
    parser.add_argument('--bbox_interval', type=int, default=-1, help='Set bounding-box image logging interval')
    parser.add_argument('--artifact_alias', type=str, default='latest', help='Version of dataset artifact to use')

    return parser.parse_known_args()[0] if known else parser.parse_args()
```

> `--rect` ‰ΩøÁî®ÈïøÊñπÂΩ¢ËÆ≠ÁªÉ
>
> Setting "rect"=True allows you to train using rectangular images, not necessarily square ones. This allows for more efficient use of GPU memory as there's less need for padding spatial dimensions.
>
> [Custom input size: letterbox vs resizing ¬∑ Issue #11350 ](https://github.com/ultralytics/yolov5/issues/11350)
>
> [About the rectangle training ¬∑ Issue #4819](https://github.com/ultralytics/ultralytics/issues/4819)

> 
>
> Â≠¶‰π†ÁéáÁöÑË∞ÉÊï¥Âú® `hyps`‰∏≠Ë∞ÉÊï¥
>
> `initial learning rate (SGD=1E-2, Adam=1E-3)`

## ÂçïÊòæÂç°ËÆ≠ÁªÉ

> `SGD`

```sh
python train.py --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 --hyp data/hyps/hyp.scratch-low.yaml  --optimizer SGD --cos-lr --device 0 --weights weights/yolov5n.pt --cfg models/yolov5n.yaml --data data/coco128.yaml
python train.py --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 --hyp data/hyps/hyp.scratch-low.yaml  --optimizer SGD --cos-lr --device 0 --weights weights/yolov5s.pt --cfg models/yolov5s.yaml --data data/coco128.yaml
python train.py --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 --hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --cos-lr --device 0 --weights weights/yolov5m.pt --cfg models/yolov5m.yaml --data data/coco128.yaml
python train.py --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 --hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --cos-lr --device 0 --weights weights/yolov5l.pt --cfg models/yolov5l.yaml --data data/coco128.yaml
python train.py --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 --hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --cos-lr --device 0 --weights weights/yolov5x.pt --cfg models/yolov5x.yaml --data data/coco128.yaml
```

> `Adam` & `AdamW`

```sh
python train.py --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 --hyp data/hyps/hyp.scratch-low-adam.yaml  --optimizer AdamW --cos-lr --device 0 --weights weights/yolov5n.pt --cfg models/yolov5n.yaml --data data/coco128.yaml
python train.py --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 --hyp data/hyps/hyp.scratch-low-adam.yaml  --optimizer AdamW --cos-lr --device 0 --weights weights/yolov5s.pt --cfg models/yolov5s.yaml --data data/coco128.yaml
python train.py --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 --hyp data/hyps/hyp.scratch-high-adam.yaml --optimizer AdamW --cos-lr --device 0 --weights weights/yolov5m.pt --cfg models/yolov5m.yaml --data data/coco128.yaml
python train.py --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 --hyp data/hyps/hyp.scratch-high-adam.yaml --optimizer AdamW --cos-lr --device 0 --weights weights/yolov5l.pt --cfg models/yolov5l.yaml --data data/coco128.yaml
python train.py --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 --hyp data/hyps/hyp.scratch-high-adam.yaml --optimizer AdamW --cos-lr --device 0 --weights weights/yolov5x.pt --cfg models/yolov5x.yaml --data data/coco128.yaml
```

## Â§öÊòæÂç°ËÆ≠ÁªÉ

- -m torch.distributed.launch pytorchÂêØÁî®Â§öÁ∫øÁ®ã
- --nproc_per_node=8      8Âº†ÊòæÂç°
- --device 0,1,2,3,4,5,6,7  8Âº†ÊòæÂç°Â∫èÂè∑

```sh
python -m torch.distributed.launch --nproc_per_node=8 train.py --device 0,1,2,3,4,5,6,7 --sync-bn --img 640 --batch-size -1 --workers 8--epochs 300 --save-period 10 \
--hyp data/hyps/hyp.scratch-low.yaml --optimizer SGD --cos-lr --weights weights/yolov5n.pt --cfg models/yolov5n.yaml --data data/coco128.yaml

python -m torch.distributed.launch --nproc_per_node=8 train.py --device 0,1,2,3,4,5,6,7 --sync-bn --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 \
--hyp data/hyps/hyp.scratch-low.yaml --optimizer SGD --cos-lr --weights weights/yolov5s.pt --cfg models/yolov5s.yaml --data data/coco128.yaml

python -m torch.distributed.launch --nproc_per_node=8 train.py --device 0,1,2,3,4,5,6,7 --sync-bn --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 \
--hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --cos-lr --weights weights/yolov5m.pt --cfg models/yolov5m.yaml --data data/coco128.yaml

python -m torch.distributed.launch --nproc_per_node=8 train.py --device 0,1,2,3,4,5,6,7 --sync-bn --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 \
--hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --cos-lr --weights weights/yolov5l.pt --cfg models/yolov5l.yaml --data data/coco128.yaml

python -m torch.distributed.launch --nproc_per_node=8 train.py --device 0,1,2,3,4,5,6,7 --sync-bn --img 640 --batch-size -1 --workers 8 --epochs 300 --save-period 10 \
--hyp data/hyps/hyp.scratch-high.yaml --optimizer SGD --cos-lr --weights weights/yolov5x.pt --cfg models/yolov5x.yaml --data data/coco128.yaml
```

## **‰∏çÈúÄË¶ÅÂú®Ê®°ÂûãÈÖçÁΩÆ‰∏≠ÊòæÁ§∫Êõ¥ÊîπÁ±ªÂà´Êï∞**

> ‰ºöËá™Âä®Â∞ÜncË∞ÉÊï¥‰∏∫Êï∞ÊçÆÈõÜÁöÑÁ±ªÂà´Êï∞Èáè

```sh
> python train.py --img 640 --batch-size -1 --epochs 300 --hyp data/hyps/hyp.scratch-low.yaml  --optimizer SGD --cos-lr --device 0 --weights weights/yolov5n.pt --cfg models/yolov5n.yaml --data data/classes20.yaml
train: weights=weights/yolov5n.pt, cfg=models/yolov5n.yaml, data=data/classes20.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=300, batch_size=-1, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\train, name=exp, exist_ok=False, quad=False, cos_lr=True, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest
remote: Enumerating objects: 10, done.
remote: Counting objects: 100% (10/10), done.
remote: Compressing objects: 100% (10/10), done.
remote: Total 10 (delta 1), reused 4 (delta 0), pack-reused 0
Unpacking objects: 100% (10/10), 5.30 KiB | 246.00 KiB/s, done.
From https://github.com/ultralytics/yolov5
   b96f35c..b54fd0a  master     -> origin/master
 * [new branch]      dependabot/github_actions/actions/stale-8 -> origin/dependabot/github_actions/actions/stale-8
github:  YOLOv5 is out of date by 1 commit. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.
YOLOv5  v7.0-128-gb96f35c Python-3.10.9 torch-2.0.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11264MiB)

hyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0
ClearML: run 'pip install clearml' to automatically track, visualize and remotely train YOLOv5  in ClearML
Comet: run 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet
TensorBoard: Start with 'tensorboard --logdir runs\train', view at http://localhost:6006/
Overriding model.yaml nc=80 with nc=20 		# ËøôÈáåËá™Âä®Ë¶ÜÁõñ‰∫ÜÊóßÁöÑÁ±ªÂà´Êï∞

                 from  n    params  module                                  arguments
  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]
  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]
  2                -1  1      4800  models.common.C3                        [32, 32, 1]
  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]
  4                -1  2     29184  models.common.C3                        [64, 64, 2]
  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
  6                -1  3    156928  models.common.C3                        [128, 128, 3]
  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]
  8                -1  1    296448  models.common.C3                        [256, 256, 1]
  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]
 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 12           [-1, 6]  1         0  models.common.Concat                    [1]
 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]
 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 16           [-1, 4]  1         0  models.common.Concat                    [1]
 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]
 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]
 19          [-1, 14]  1         0  models.common.Concat                    [1]
 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]
 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]
 22          [-1, 10]  1         0  models.common.Concat                    [1]
 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]
 24      [17, 20, 23]  1     33825  models.yolo.Detect                      [20, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]
YOLOv5n summary: 214 layers, 1790977 parameters, 1790977 gradients, 4.3 GFLOPs
```

> Ëá™Âä®Ë∞ÉÊï¥ `nc` ÁöÑ‰ª£Á†ÅÂú® `models/yolo.py`

```python
        if nc and nc != self.yaml['nc']: # ‰ΩøÁî®data config‰∏≠ÁöÑnamesÈïøÂ∫¶Ë¶ÜÁõñÊ®°ÂûãÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑÁ±ªÂà´
            LOGGER.info(f"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}")
            self.yaml['nc'] = nc  # override yaml value
```

## ËÆ≠ÁªÉÊó∂Âá∫Áé∞ÁöÑÈóÆÈ¢ò

### ËÆ≠ÁªÉ `obj_loss` Â¢ûÂ§ß | reduce FPs | Ëß£ÂÜ≥ÁâπÊÆäÂú∫ÊôØÊ®°ÂûãÊãçÊëÑÊó•Â∏∏ÁõÆÊ†áÁöÑFPÊï∞ÈáèËøáÂ§ö

> [how to use Background images in training? ¬∑ Issue #2844 ¬∑ ultralytics/yolov5 (github.com)](https://github.com/ultralytics/yolov5/issues/2844)
>
> Âú®ÂõæÁâáËÆ≠ÁªÉÊñá‰ª∂Â§π `images/train` ‰∏≠Ê∑ªÂä†ËÉåÊôØÂõæÁâáÊñá‰ª∂ÔºåÊØîÂ¶ÇcocoÊàñËÄÖvocÊï∞ÊçÆÈõÜÁöÑ‰∏Ä‰∫õÁÖßÁâá
>
> ‰∏çÈúÄË¶ÅÊ∑ªÂä†Á©∫ÁôΩlabel txtÊñá‰ª∂ÔºåÊ∑ªÂä†‰∫Ü‰πü‰∏ç‰ºöÂá∫Èîô
>
> `(if no objects in image, no `*.txt` file is required).`
>
> [ÁõÆÊ†áÊ£ÄÊµãÔºàÈôç‰ΩéËØØÊ£ÄÊµãÁéáÂèäÂ∞èÁõÆÊ†áÊ£ÄÊµãÁ≥ªÂàóÁ¨îËÆ∞Ôºâ](https://blog.csdn.net/weixin_44836143/article/details/105952819)

```sh
train: Scanning D:\code\datasets\classes20\labels\train... 5266 images, 1000 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|
train: New cache created: D:\code\datasets\classes20\labels\train.cache
val: Scanning D:\code\datasets\classes20\labels\val... 586 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|
val: New cache created: D:\code\datasets\classes20\labels\val.cache
```

# export

```python
"""
Export a YOLOv5 PyTorch model to other formats. TensorFlow exports authored by https://github.com/zldrobit

Format                      | `export.py --include`         | Model
---                         | ---                           | ---
PyTorch                     | -                             | yolov5s.pt
TorchScript                 | `torchscript`                 | yolov5s.torchscript
ONNX                        | `onnx`                        | yolov5s.onnx
OpenVINO                    | `openvino`                    | yolov5s_openvino_model/
TensorRT                    | `engine`                      | yolov5s.engine
CoreML                      | `coreml`                      | yolov5s.mlmodel
TensorFlow SavedModel       | `saved_model`                 | yolov5s_saved_model/
TensorFlow GraphDef         | `pb`                          | yolov5s.pb
TensorFlow Lite             | `tflite`                      | yolov5s.tflite
TensorFlow Edge TPU         | `edgetpu`                     | yolov5s_edgetpu.tflite
TensorFlow.js               | `tfjs`                        | yolov5s_web_model/
PaddlePaddle                | `paddle`                      | yolov5s_paddle_model/

Requirements:
    $ pip install -r requirements.txt coremltools onnx onnxsim onnxruntime openvino-dev tensorflow-cpu  # CPU
    $ pip install -r requirements.txt coremltools onnx onnxsim onnxruntime-gpu openvino-dev tensorflow  # GPU

Usage:
    $ python export.py --weights yolov5s.pt --include torchscript onnx openvino engine coreml tflite ...

Inference:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle

TensorFlow.js:
    $ cd .. && git clone https://github.com/zldrobit/tfjs-yolov5-example.git && cd tfjs-yolov5-example
    $ npm install
    $ ln -s ../../yolov5/yolov5s_web_model public/yolov5s_web_model
    $ npm start
"""

def parse_opt(known=False):
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model.pt path(s)')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640, 640], help='image (h, w)')
    parser.add_argument('--batch-size', type=int, default=1, help='batch size')
    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')
    parser.add_argument('--inplace', action='store_true', help='set YOLOv5 Detect() inplace=True')
    parser.add_argument('--keras', action='store_true', help='TF: use Keras')
    parser.add_argument('--optimize', action='store_true', help='TorchScript: optimize for mobile')
    parser.add_argument('--int8', action='store_true', help='CoreML/TF INT8 quantization')
    parser.add_argument('--dynamic', action='store_true', help='ONNX/TF/TensorRT: dynamic axes')
    parser.add_argument('--simplify', action='store_true', help='ONNX: simplify model')
    parser.add_argument('--opset', type=int, default=17, help='ONNX: opset version')
    parser.add_argument('--verbose', action='store_true', help='TensorRT: verbose log')
    parser.add_argument('--workspace', type=int, default=4, help='TensorRT: workspace size (GB)')
    parser.add_argument('--nms', action='store_true', help='TF: add NMS to model')
    parser.add_argument('--agnostic-nms', action='store_true', help='TF: add agnostic NMS to model')
    parser.add_argument('--topk-per-class', type=int, default=100, help='TF.js NMS: topk per class to keep')
    parser.add_argument('--topk-all', type=int, default=100, help='TF.js NMS: topk for all classes to keep')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='TF.js NMS: IoU threshold')
    parser.add_argument('--conf-thres', type=float, default=0.25, help='TF.js NMS: confidence threshold')
    parser.add_argument(
        '--include',
        nargs='+',
        default=['torchscript'],
        help='torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle')
    opt = parser.parse_known_args()[0] if known else parser.parse_args()
    print_args(vars(opt))
    return opt
```

> ÂØºÂá∫Ë∑ØÂæÑÂíåÊùÉÈáçË∑ØÂæÑÁõ∏Âêå
>
> --include ÂêéÈù¢ÂÜôÊÉ≥ÂØºÂá∫ÁöÑÊ†ºÂºè

## torchscript

```sh
python export.py --imgsz 640 --weights weights/yolov5s.pt --include torchscript --device 0
python export.py --imgsz 640 --weights weights/yolov5s.pt --include torchscript --device cpu --optimize # --optimize not compatible with cuda devices, i.e. use --device cpu
```

## onnx

> Ê≥®ÊÑè:
>
> `onnxruntime` Âíå `onnxruntime-gpu` ‰∏çË¶ÅÂêåÊó∂ÂÆâË£ÖÔºåÂê¶Âàô‰ΩøÁî® `gpu` Êé®ÁêÜÊó∂ÈÄüÂ∫¶‰ºöÂæàÊÖ¢ÔºåÂ¶ÇÊûúÂêåÊó∂ÂÆâË£Ö‰∫Ü2‰∏™ÂåÖÔºåË¶ÅÂÖ®ÈÉ®Âç∏ËΩΩÔºåÂÜçÂÆâË£Ö 'onnxruntime-gpu' ÊâçËÉΩ‰ΩøÁî®gpuÊé®ÁêÜÔºåÂê¶ÂàôgpuÈÄüÂ∫¶‰ºöÂæàÊÖ¢

```sh
python export.py --imgsz 640 --weights weights/yolov5s.pt --include onnx --simplify --device 0

python export.py --imgsz 640 --weights weights/yolov5s.pt --include onnx --simplify --device 0 --half      			# --half only compatible with GPU export, i.e. use --device 0

python export.py --imgsz 640 --weights weights/yolov5s.pt --include onnx --simplify --device cpu --dynamic 			# --dynamic only compatible with cpu

python export.py --imgsz 640 --weights weights/yolov5s.pt --include onnx --simplify --device cpu --half --dynamic	# ÂØºÂá∫Â§±Ë¥• --half not compatible with --dynamic
```

### opencv‰ΩøÁî®ÁöÑonnx

> https://github.com/ultralytics/ultralytics/tree/main/examples/YOLOv8-OpenCV-ONNX-Python

```sh
python export.py --imgsz 640 --weights weights/yolov5s.pt --include onnx --simplify --device 0 --opset 12			# opsetÂøÖÈ°ª‰∏∫12

python export.py --imgsz 640 --weights weights/yolov5s.pt --include onnx --simplify --device 0 --half --opset 12	# opsetÂøÖÈ°ª‰∏∫12

# opencv‰∏çÊîØÊåÅdynamic
```

## openvino

```sh
python export.py --imgsz 640 --weights weights/yolov5s.pt --include openvino --simplify --device cpu      # ÂèØ‰ª•Áî®simplifyÁöÑonnx
python export.py --imgsz 640 --weights weights/yolov5s.pt --include openvino --simplify --device 0 --half # openvinoÊîØÊåÅhalf,‰ΩÜÊòØË¶Å‰ΩøÁî®cpuÂØºÂá∫onnxÁöÑhalf‰ºöÊä•Èîô,ÊâÄ‰ª•Ë¶Å‰ΩøÁî® --device 0, openvinoÂØºÂá∫ÂíåËÆæÂ§áÊó†ÂÖ≥,‰∏çÂèóÂΩ±Âìç,‰∏ªË¶ÅÊòØÂØºÂá∫onnxÁöÑÈóÆÈ¢ò

python export.py --imgsz 640 --weights weights/yolov5s.pt --include openvino --simplify --device cpu --int8 # ÈúÄË¶ÅÂÆâË£Önncf
```

### ÈÄöËøáopenvinoÁöÑ`mo`ÂëΩ‰ª§Â∞ÜonnxËΩ¨Êç¢‰∏∫openvinoÊ†ºÂºè(ÊîØÊåÅ**fp16**)

> https://docs.openvino.ai/latest/notebooks/102-pytorch-onnx-to-openvino-with-output.html

```sh
mo --input_model "onnx_path" --output_dir "output_path" --compress_to_fp16

mo --input_model "onnx_path" --output_dir "output_path" --compress_to_fp16
```

#### ‰ª£Á†ÅÊñπÂºè

```python
from openvino.tools import mo
from openvino.runtime import serialize

onnx_path = "onnx_path"

# fp32 IR model
fp32_path = "fp32_path"
output_path = fp32_path + ".xml"
print(f"Export ONNX to OpenVINO FP32 IR to: {output_path}")
model = mo.convert_model(onnx_path)
serialize(model, output_path)

# fp16 IR model
fp16_path = "fp16_path"
output_path = fp16_path + ".xml"

print(f"Export ONNX to OpenVINO FP16 IR to: {output_path}")
model = mo.convert_model(onnx_path, compress_to_fp16=True)
serialize(model, output_path)
```

### export failure  0.9s: DLL load failed while importing ie_api

> https://blog.csdn.net/qq_26815239/article/details/123047840
>
> Â¶ÇÊûú‰Ω†‰ΩøÁî®ÁöÑÊòØ Python 3.8 ÊàñÊõ¥È´òÁâàÊú¨ÔºåÂπ∂‰∏îÊòØÂú®WindowsÁ≥ªÁªü‰∏ãÈÄöËøápipÂÆâË£ÖÁöÑopenvinoÔºåÈÇ£‰πàËØ•ÈîôËØØÁöÑËß£ÂÜ≥ÊñπÊ°àÂ¶Ç‰∏ãÔºö

1. ËøõÂÖ•ÁõÆÂΩï `your\env\site-packages\openvino\inference_engine`
2. ÊâìÂºÄÊñá‰ª∂ `__init__.py`
3. 26Ë°å‰∏ãÊ∑ªÂä†‰∏ÄË°å

```python
        if os.path.isdir(lib_path):
            # On Windows, with Python >= 3.8, DLLs are no longer imported from the PATH.
            if (3, 8) <= sys.version_info:
                os.add_dll_directory(os.path.abspath(lib_path))
                os.environ['PATH'] = os.path.abspath(lib_path) + ';' + os.environ['PATH']	# Ê∑ªÂä†Ëøô‰∏ÄË°å
```

## tensorrt

```sh
python export.py --imgsz 640 --weights weights/yolov5s.pt --include engine --simplify --device 0 # ÂèØ‰ª•Áî®simplifyÁöÑonnx

python export.py --imgsz 640 --weights weights/yolov5s.pt --include engine --simplify --device 0 --half

python export.py --imgsz 640 --weights weights/yolov5s.pt --include engine --simplify --device 0 --dynamic --batch-size=16 	       # --dynamic model requires maximum --batch-size argument

python export.py --imgsz 640 --weights weights/yolov5s.pt --include engine --simplify --device 0 --half --dynamic --batch-size=16  # ÂØºÂá∫Â§±Ë¥• --half not compatible with --dynamic, i.e. use either --half or --dynamic but not both
```

## onnx openvino tensorrt

```sh
python export.py --imgsz 640 --weights weights/yolov5s.pt --include onnx openvino engine --simplify --device 0 --half 
```

# detect

```python
"""
Run YOLOv5 detection inference on images, videos, directories, globs, YouTube, webcam, streams, etc.

Usage - sources:
    $ python detect.py --weights yolov5s.pt --source 0                               # webcam
                                                     img.jpg                         # image
                                                     vid.mp4                         # video
                                                     screen                          # screenshot
                                                     path/                           # directory
                                                     list.txt                        # list of images
                                                     list.streams                    # list of streams
                                                     'path/*.jpg'                    # glob
                                                     'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                                     'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle
"""

def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path or triton URL')
    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob/screen/0(webcam)')
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')
    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='show results')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--visualize', action='store_true', help='visualize features')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')
    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')
    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    parser.add_argument('--vid-stride', type=int, default=1, help='video frame-rate stride')
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand
    print_args(vars(opt))
    return opt
```

> sourceÂêéÈù¢ÂèØ‰ª•ÊîæÂõæÁâáÔºåËßÜÈ¢ëÊàñËÄÖÊñá‰ª∂Â§πË∑ØÂæÑÔºå‰ºö‰øùÂ≠òÂà∞runs/detectÁõÆÂΩï‰∏ãÈù¢

`python detect.py --weights ÊùÉÈáçË∑ØÂæÑ --source ÂõæÁâáorËßÜÈ¢ëorÊñá‰ª∂Â§πË∑ØÂæÑ`



```python
# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license
"""
Run YOLOv5 detection inference on images, videos, directories, globs, YouTube, webcam, streams, etc.

Usage - sources:
    $ python detect.py --weights yolov5s.pt --source 0                               # webcam
                                                     img.jpg                         # image
                                                     vid.mp4                         # video
                                                     screen                          # screenshot
                                                     path/                           # directory
                                                     list.txt                        # list of images
                                                     list.streams                    # list of streams
                                                     'path/*.jpg'                    # glob
                                                     'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                                     'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle
"""
```

## torch

```sh
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.pt --source data/images/bus.jpg --device 0

python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.pt --source ../datasets/coco128/images/train2017 --device 0
```

## torchscript

```sh
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.torchscript --source data/images/bus.jpg --device 0

python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.torchscript --source ../datasets/coco128/images/train2017 --device 0
```

## onnx

> Ê≥®ÊÑè:
>
> `onnxruntime` Âíå `onnxruntime-gpu` ‰∏çË¶ÅÂêåÊó∂ÂÆâË£ÖÔºåÂê¶Âàô‰ΩøÁî® `gpu` Êé®ÁêÜÊó∂ÈÄüÂ∫¶‰ºöÂæàÊÖ¢ÔºåÂ¶ÇÊûúÂêåÊó∂ÂÆâË£Ö‰∫Ü2‰∏™ÂåÖÔºåË¶ÅÂÖ®ÈÉ®Âç∏ËΩΩÔºåÂÜçÂÆâË£Ö `onnxruntime-gpu` ÊâçËÉΩ‰ΩøÁî®gpuÊé®ÁêÜÔºåÂê¶ÂàôgpuÈÄüÂ∫¶‰ºöÂæàÊÖ¢

```sh
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.onnx --source data/images/bus.jpg --device 0
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.onnx --source ../datasets/coco128/images/train2017 --device 0

python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.fp16.onnx --half --source data/images/bus.jpg --device 0				# fp16Ê®°ÂûãÈúÄË¶Å --half
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.fp16.onnx --half --source ../datasets/coco128/images/train2017 --device 0

python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.fp16.onnx --half --source data/images/bus.jpg --device cpu				# gpuÂØºÂá∫ÁöÑfp16Ê®°ÂûãÂèØ‰ª•Áî®cpuÊé®ÁêÜ
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.fp16.onnx --half --source ../datasets/coco128/images/train2017 --device cpu

python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.cpu.dynamic.onnx --source data/images/bus.jpg --device 0				# ‰ΩøÁî®cpuÂØºÂá∫ÁöÑdynamicÊ®°ÂûãÂèØ‰ª•Áî®gpuÊé®ÁêÜ
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.cpu.dynamic.onnx --source ../datasets/coco128/images/train2017 --device 0
```

## openvino

> Ê≥®ÊÑèÔºöopenvinoÊ≤°Ê≥ï‰ΩøÁî®cudaÔºå‰ΩÜÊòØ‰ΩøÁî® --device 0 ‰ºöÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶

```sh
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s_openvino_model --source data/images/bus.jpg --device cpu

python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s_openvino_model --source ../datasets/coco128/images/train2017 --device cpu
```

## tensorrt

```sh
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.engine --half --source data/images/bus.jpg --device 0					# fp32Ê®°Âûã‰πüËÉΩÁî® --half Êé®ÁêÜ
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.engine --half --source ../datasets/coco128/images/train2017 --device 0

python detect.py --imgsz 640 --save-txt --save-conf --save-crop --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.fp16.engine --half --source data/images/bus.jpg --device 0
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.fp16.engine --half --source ../datasets/coco128/images/train2017 --device 0

python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.fp32.dynamic.engine --half --source data/images/bus.jpg --device 0		 # fp32Ê®°Âûã‰πüËÉΩÁî® --half Êé®ÁêÜ
python detect.py --imgsz 640 --save-txt --save-conf --save-crop --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.fp32.dynamic.engine --half --source ../datasets/coco128/images/train2017 --device 0
```

# val

```python
"""
Validate a trained YOLOv5 detection model on a detection dataset

Usage:
    $ python val.py --weights yolov5s.pt --data coco128.yaml --img 640

Usage - formats:
    $ python val.py --weights yolov5s.pt                 # PyTorch
                              yolov5s.torchscript        # TorchScript
                              yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                              yolov5s_openvino_model     # OpenVINO
                              yolov5s.engine             # TensorRT
                              yolov5s.mlmodel            # CoreML (macOS-only)
                              yolov5s_saved_model        # TensorFlow SavedModel
                              yolov5s.pb                 # TensorFlow GraphDef
                              yolov5s.tflite             # TensorFlow Lite
                              yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                              yolov5s_paddle_model       # PaddlePaddle
"""

def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')
    parser.add_argument('--batch-size', type=int, default=32, help='batch size')
    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.001, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.6, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=300, help='maximum detections per image')
    parser.add_argument('--task', default='val', help='train, val, test, speed or study')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')
    parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--verbose', action='store_true', help='report mAP by class')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-hybrid', action='store_true', help='save label+prediction hybrid results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-json', action='store_true', help='save a COCO-JSON results file')
    parser.add_argument('--project', default=ROOT / 'runs/val', help='save to project/name')
    parser.add_argument('--name', default='exp', help='save to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    opt = parser.parse_args()
    opt.data = check_yaml(opt.data)  # check YAML
    opt.save_json |= opt.data.endswith('coco.yaml')
    opt.save_txt |= opt.save_hybrid
    print_args(vars(opt))
    return opt
```

## default confidence threshold = 0.001

> [mAP bug at higher --conf ¬∑ Issue #1466 ¬∑ ultralytics/yolov5](https://github.com/ultralytics/yolov5/issues/1466)
>
> [Why does the confidence threshold of 0.001 in val.py result in good results? ¬∑ Issue #11745 ¬∑ ultralytics/yolov5](https://github.com/ultralytics/yolov5/issues/11745)

## È™åËØÅÊ®°ÂûãÂú®Ëá™ÂÆö‰πâÊï∞ÊçÆÈõÜ‰∏äÁöÑÊïàÊûú Á≤æÂ∫¶0.995

> https://www.jianshu.com/p/cfb01add61bd#1684051613808
>
> https://github.com/ultralytics/yolov5/issues/5508
>
> https://github.com/ultralytics/yolov5/issues/1563
>
> https://github.com/ultralytics/yolov5/pull/1646
>
>  `--save-hybrid` ‰ºöÂêàÂπ∂Â∑≤Áü•ÁöÑlabelsÔºåÂØºËá¥ÂæóÂàÜÂæàÈ´ò

## torch

```sh
python val.py --imgsz 640 --save-txt --save-conf --save-json --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.pt --device 0
```

## torchscript

```sh
python val.py --imgsz 640 --save-txt --save-conf --save-json --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.torchscript --device 0
```

## onnx

> Ê≥®ÊÑè:
>
> `onnxruntime` Âíå `onnxruntime-gpu` ‰∏çË¶ÅÂêåÊó∂ÂÆâË£ÖÔºåÂê¶Âàô‰ΩøÁî® `gpu` Êé®ÁêÜÊó∂ÈÄüÂ∫¶‰ºöÂæàÊÖ¢ÔºåÂ¶ÇÊûúÂêåÊó∂ÂÆâË£Ö‰∫Ü2‰∏™ÂåÖÔºåË¶ÅÂÖ®ÈÉ®Âç∏ËΩΩÔºåÂÜçÂÆâË£Ö `onnxruntime-gpu` ÊâçËÉΩ‰ΩøÁî®gpuÊé®ÁêÜÔºåÂê¶ÂàôgpuÈÄüÂ∫¶‰ºöÂæàÊÖ¢

```sh
python val.py --imgsz 640 --save-txt --save-conf --save-json --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.onnx --device 0

python val.py --imgsz 640 --save-txt --save-conf --save-json --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.onnx --device 0 --dnn
```

## openvino

> Ê≥®ÊÑèÔºöopenvinoÊ≤°Ê≥ï‰ΩøÁî®cudaÔºå‰ΩÜÊòØ‰ΩøÁî® --device 0 ‰ºöÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶

```sh
python val.py --imgsz 640 --save-txt --save-conf --save-json --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s_openvino_model --device cpu
```

## tensorrt

```sh
python val.py --imgsz 640 --save-txt --save-conf --save-json --conf-thres 0.25 --iou-thres 0.6 --data data/coco128.yaml --weights weights/yolov5s.engine --device 0
```

